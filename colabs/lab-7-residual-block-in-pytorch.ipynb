{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
        "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement residual connections in the missing places in the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
        "\n",
        "    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gmPxKmU4h6kx"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward pass.\n",
        "        fx = self.conv_block_1(x)\n",
        "        fx = self.conv_block_2(fx)\n",
        "        if self.in_channels == self.out_channels:\n",
        "            x_prime = x\n",
        "        elif self.in_channels < self.out_channels:\n",
        "            padding = torch.zeros(x.shape[2], x.shape[3]).unsqueeze(0).unsqueeze(0)\n",
        "            padding_tensor = padding.repeat(x.shape[0], self.out_channels - self.in_channels, 1, 1)\n",
        "            x_prime = torch.cat((x, padding_tensor), dim=1)\n",
        "        return nn.functional.relu(torch.add(fx, x_prime))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "a99VndsNh6kz"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.rc = nn.Sequential(\n",
        "            ResidualConnection(1, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            28 * 28 * 16, 10\n",
        "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rc(x)\n",
        "        x = nn.Flatten(start_dim=1)(x)\n",
        "        x = self.fc(x)\n",
        "        output = nn.LogSoftmax(dim=1)(x)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "K5GlMs1-fBKP"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "test_batch_size = 1000\n",
        "epochs = 3\n",
        "lr = 1e-2\n",
        "use_cuda = False\n",
        "seed = 1\n",
        "log_interval = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "use_cuda = not use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "o0KPoUtsfBOs"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        ")\n",
        "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvIQbgsfBRT",
        "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.526710\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 14.373219\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.369129\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.514152\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.605485\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.821124\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.372500\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.310326\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.356186\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.220788\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.364109\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.325376\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.199410\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.170073\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.244049\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.174176\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.142816\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.118412\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.325115\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.076910\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090124\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.144967\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.108086\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.004871\n",
            "\n",
            "Test set: Average loss: 0.1128, Accuracy: 9679/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.140987\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.144127\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.142815\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.151943\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.082626\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.140874\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.084056\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.095323\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.110316\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.085478\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.117459\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.115673\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.091578\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.077943\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.079037\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.071023\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.063586\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.058669\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.187577\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.045880\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.059320\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.084075\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.043955\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001871\n",
            "\n",
            "Test set: Average loss: 0.0808, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.083962\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.118471\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.080559\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.042756\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.067784\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.063195\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.051629\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.061907\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.076887\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.028092\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.076637\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.086007\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.070612\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.050848\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.040639\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.062000\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.038126\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.043793\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.104820\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.031045\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.036089\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.055575\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.023053\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.000952\n",
            "\n",
            "Test set: Average loss: 0.0718, Accuracy: 9786/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DQMSSwuifBTo"
      },
      "outputs": [],
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.rc = nn.Sequential(\n",
        "            ResidualConnection(1, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            \n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            \n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            \n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            28 * 28 * 16, 10\n",
        "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rc(x)\n",
        "        x = nn.Flatten(start_dim=1)(x)\n",
        "        x = self.fc(x)\n",
        "        output = nn.LogSoftmax(dim=1)(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JX_2rCycfBWU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 4.266386\n",
            "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 36.106873\n",
            "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 4.755883\n",
            "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 5.631119\n",
            "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 3.658088\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.227066\n",
            "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 2.833503\n",
            "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 2.367362\n",
            "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 1.846981\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     test(model, device, test_loader)\n",
            "Cell \u001b[0;32mIn[36], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/gsn-JLMYFmkM/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/gsn-JLMYFmkM/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = Net2().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('gsn-JLMYFmkM')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b24dd471e30d41a13314bcb0a6607aa895817efc34b906f957cb683f2502811d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
