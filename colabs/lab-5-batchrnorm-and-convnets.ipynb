{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfdcY0Vq6e80"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "This exercise covers two aspects:\n",
        "* In tasks 1-6 you will implement mechanisms that allow training deeper models (better initialization, batch normalization). Note that for dropout and batch norm you are expected to implement it yourself without relying on ready-made components from Pytorch.\n",
        "* In task 7 you will implement a convnet using [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
        "\n",
        "\n",
        "Tasks:\n",
        "1. Check that the given implementation reaches 95% test accuracy for\n",
        "   architecture input-64-64-10 in a few thousand batches.\n",
        "2. Improve initialization and check that the network learns much faster\n",
        "   and reaches over 97% test accuracy. A good basic initialization scheme is so-called Glorot initialization. For a set of weights going from a layer with $n_{in}$ neurons to a layer with $n_{out}$ neurons, it samples each weight from normal distribution with $0$ mean and standard deviation of $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$.\n",
        "3. Check, that with proper initialization we can train architecture\n",
        "   input-64-64-64-64-64-10, while with bad initialization it does\n",
        "   not even get off the ground.\n",
        "4. Add dropout implemented in pytorch\n",
        "5. Check that with 10 hidden layers (64 units each) even with proper\n",
        "    initialization the network has a hard time to start learning.\n",
        "6. Implement batch normalization (use train mode also for testing - it should perform well enough):\n",
        "    * compute batch mean and variance\n",
        "    * add new variables beta and gamma\n",
        "    * check that the networks learns much faster for 5 layers\n",
        "    * check that the network learns even for 10 hidden layers.\n",
        "7. So far we worked with a fully connected network. Design and implement in pytorch (by using pytorch functions) a simple convolutional network and achieve 99% test accuracy. The architecture is up to you, but even a few convolutional layers should be enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.19.2\n",
            "3.10.8 (main, Nov  1 2022, 17:01:49) [GCC 12.2.0]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import ipykernel\n",
        "import sys\n",
        "print(ipykernel.__version__)\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.weight.data.normal_(mean=0,std=0.25)\n",
        "        init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = x.matmul(self.weight.t())\n",
        "        r += self.bias\n",
        "        return r\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = Linear(784, 64)\n",
        "        self.fc2 = Linear(64, 64)\n",
        "        self.fc3 = Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "class MnistTrainer(object):\n",
        "    def __init__(self, batch_size, net, epochs=1):\n",
        "        transform = transforms.Compose(\n",
        "                [transforms.ToTensor()])\n",
        "        self.trainset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            download=True,\n",
        "            train=True,\n",
        "            transform=transform)\n",
        "        self.trainloader = torch.utils.data.DataLoader(\n",
        "            self.trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        self.testset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True, transform=transform)\n",
        "        self.testloader = torch.utils.data.DataLoader(\n",
        "            self.testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "        self.net = net\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def train(self):\n",
        "        net = self.net\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            running_loss = 0.0\n",
        "            net.train()\n",
        "            for i, data in enumerate(self.trainloader, 0):\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                if i % 100 == 99:\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 100))\n",
        "                    running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            net.eval()\n",
        "            with torch.no_grad():\n",
        "                for data in self.testloader:\n",
        "                    images, labels = data\n",
        "                    outputs = net(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                total, 100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "DQMSSwuifBTo"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "class LinearGlorot(Linear):\n",
        "    def reset_parameters(self):\n",
        "        std = sqrt(2.0 / (self.in_features + self.out_features))\n",
        "        self.weight.data.normal_(mean=0,std=std)\n",
        "        init.zeros_(self.bias)\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, sizes, linearClass, p=0.0):\n",
        "        super(MyNet, self).__init__()\n",
        "        # After flattening an image of size 28x28 we have 784 inputs\n",
        "        self.p = p\n",
        "        self.fcs = nn.ModuleList([linearClass(a, b) for a, b in zip([784]+sizes, sizes+[10])])\n",
        "\n",
        "    def dropout(self, x):\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return x\n",
        "        dropout = torch.from_numpy(np.random.choice([0, 1.0/(1-self.p)], size=x.shape, p=[self.p, 1-self.p]))\n",
        "        return torch.mul(x, dropout.float())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        for fc in self.fcs[:-1]:\n",
        "            x = fc(x)\n",
        "            x = self.dropout(x)\n",
        "            x = F.relu(x)\n",
        "        x = self.fcs[-1](x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "JX_2rCycfBWU"
      },
      "outputs": [],
      "source": [
        "def better_initialization():\n",
        "    print(\"Bad initialization, sizes=[784,64,64,10]\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64,64], Linear)).train()\n",
        "    print(\"Glorot initialization, sizes=[784,64,64,10]\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64,64], LinearGlorot)).train()\n",
        "    print(\"Bad initialization, sizes=[784,64,64,64,64,64,10]\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64,64,64,64,64], Linear)).train()\n",
        "    print(\"Glorot initialization, sizes=[784,64,64,64,64,64,10]\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64,64,64,64,64], LinearGlorot)).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "def longer_networks_should_not_learn():\n",
        "    print(\"Checking whether network with 10 hidden layers will have trouble starting learning\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64]*10, LinearGlorot)).train()\n",
        "    print(\"It does not seem to have problems, lets check 100 layers\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64]*20, LinearGlorot)).train()\n",
        "    print(\"Now it does not get off the ground\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearBN(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearBN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.a = Parameter(torch.Tensor(1))\n",
        "        self.b = Parameter(torch.Tensor(1))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = sqrt(2.0 / (self.in_features + self.out_features))\n",
        "        self.weight.data.normal_(mean=0,std=std)\n",
        "        init.zeros_(self.bias)\n",
        "        self.a.data[0] = 1\n",
        "        self.b.data[0] = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = x.matmul(self.weight.t())\n",
        "        r += self.bias\n",
        "        std = torch.std(r, dim=0)\n",
        "        r = (r - torch.mean(r, dim=0)) / torch.maximum(std, torch.full_like(std, 1e-12))\n",
        "        return self.a * r + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_batch_normalization(l):\n",
        "    print(\"length: \", l)\n",
        "    print(\"With batch normalization\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64]*l, LinearBN)).train()\n",
        "    print(\"Without batch normalization\")\n",
        "    MnistTrainer(batch_size=128, net=MyNet([64]*l, LinearGlorot)).train()\n",
        "    #TODO validation of batch normalized network, normalisation layers should use running statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCNN, self).__init__()\n",
        "        self.seq = nn.Sequential(      # 1x28x28\n",
        "            nn.Conv2d(1,4,3),          # 4x26x26\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.AvgPool2d(2,2),         # 4x13x13\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(4,8,3),          # 8x11x11\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2,1),         # 8x10x10\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8,16,3),         # 16x8x8\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2,2),         # 16x4x4\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Flatten(),              # 16*4*4\n",
        "\n",
        "            nn.Linear(16*4*4, 128),    # 128\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, 64),        # 64\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Linear(64, 32),         # 32\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(32, 10),         # 10\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_conv():\n",
        "    MnistTrainer(batch_size=128, net=MyCNN(), epochs=3).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 0.379\n",
            "[1,   200] loss: 0.095\n",
            "[1,   300] loss: 0.077\n",
            "[1,   400] loss: 0.063\n",
            "Accuracy of the network on the 10000 test images: 98.48 %\n",
            "[2,   100] loss: 0.047\n",
            "[2,   200] loss: 0.047\n",
            "[2,   300] loss: 0.048\n",
            "[2,   400] loss: 0.042\n",
            "Accuracy of the network on the 10000 test images: 99.04 %\n",
            "[3,   100] loss: 0.032\n",
            "[3,   200] loss: 0.031\n",
            "[3,   300] loss: 0.035\n",
            "[3,   400] loss: 0.036\n",
            "Accuracy of the network on the 10000 test images: 98.71 %\n"
          ]
        }
      ],
      "source": [
        "#better_initialization()\n",
        "#longer_networks_should_not_learn()\n",
        "#compare_batch_normalization(10)\n",
        "#compare_batch_normalization(20)\n",
        "#compare_batch_normalization(100) #TODO why here result is nan\n",
        "train_conv()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('gsn-JLMYFmkM')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Nov  1 2022, 17:01:49) [GCC 12.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b24dd471e30d41a13314bcb0a6607aa895817efc34b906f957cb683f2502811d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
