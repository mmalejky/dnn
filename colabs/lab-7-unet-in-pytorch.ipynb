{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0g--yyKh64a"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [UNet-like](https://arxiv.org/pdf/1505.04597.pdf) architecture for the semantic segmentation task. \n",
        "The model is trained on the [Pascal VOC](https://paperswithcode.github.io/torchbench/pascalvoc/) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement the missing pieces in the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 68% test accuracy after a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "QfIXmJ-dRXfE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "FxGS_WsORXfF"
      },
      "outputs": [],
      "source": [
        "class UNetConvolutionStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UNetConvolutionStack, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "lyN2g-yQRXfG"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, first_layer=False):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        if first_layer:\n",
        "            self.down = nn.Sequential(\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "        else:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.MaxPool2d((2, 2)),\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "dp2-OwXORXfG"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(\n",
        "            in_channel, in_channel, 3, stride=2, padding=1\n",
        "        )\n",
        "        self.up = nn.Sequential(\n",
        "            UNetConvolutionStack(in_channel + out_channel, out_channel),\n",
        "            UNetConvolutionStack(out_channel, out_channel),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # TODO: implement skipconnections.\n",
        "        # hint: x is the output of previous decoder layer,\n",
        "        # y is the output of corresponding encoder layer.\n",
        "        # Based on the arguments of the constructor,\n",
        "        # how should x and y be combined?\n",
        "        t = self.upsample(x)\n",
        "        t = torchvision.transforms.functional.crop(t, 0, 0, y.shape[2], y.shape[3])\n",
        "        combined = torch.cat((t, y), dim=1)\n",
        "        return self.up(combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "RBPeqMNSRXfG"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, encoder_channels, decoder_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.conv = nn.Conv2d(\n",
        "            decoder_channels[-1], num_classes, kernel_size=3, padding=1\n",
        "        )\n",
        "\n",
        "        encoder_sizes = zip(\n",
        "            range(len(encoder_channels)), encoder_channels, encoder_channels[1:]\n",
        "        )\n",
        "        for idx, in_size, out_size in encoder_sizes:\n",
        "            if idx > 0:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size))\n",
        "            else:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size, first_layer=True))\n",
        "\n",
        "        decoder_sizes = zip(decoder_channels, decoder_channels[1:])\n",
        "        for in_size, out_size in decoder_sizes:\n",
        "            self.decoder.append(DecoderStack(in_size, out_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement UNet's forward pass.\n",
        "        # hint: Remeber to store outputs of subsequent\n",
        "        # encoder layers to use as input to decoder layers!\n",
        "        # Do not forget about the final convolution.\n",
        "        encoder_outputs = []\n",
        "        for enc_layer in self.encoder:\n",
        "            x = enc_layer(x)\n",
        "            x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
        "            encoder_outputs.append(x)\n",
        "        encoder_outputs.pop()\n",
        "        for dec_layer in self.decoder:\n",
        "            x = dec_layer(x, encoder_outputs.pop())\n",
        "            x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "5AKH3oUqRXfH"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    _, _, image_width, image_height = data.size()\n",
        "    test_loss /= len(test_loader.dataset) * image_width * image_height\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            (len(test_loader.dataset) * image_width * image_height),\n",
        "            100.0 * correct / (len(test_loader.dataset) * image_width * image_height),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Ed1Rwhv-RXfH"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "test_batch_size = 1000\n",
        "epochs = 10\n",
        "lr = 1e-2\n",
        "use_cuda = True\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "num_classes = 22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "Ht3SPPVlRXfH"
      },
      "outputs": [],
      "source": [
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "dD85qSzwRXfI"
      },
      "outputs": [],
      "source": [
        "def replace_tensor_value_(tensor, a, b):\n",
        "    tensor[tensor == a] = b\n",
        "    return tensor\n",
        "\n",
        "\n",
        "input_resize = transforms.Resize((224, 224))\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_resize = transforms.Resize((224, 224), interpolation=InterpolationMode.NEAREST)\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(\n",
        "            lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7r9wpzeRXfI",
        "outputId": "38b20f20-c2a4-4e23-d289-fdadc299b5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ../data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ../data/VOCtrainval_11-May-2012.tar to ../data\n",
            "Using downloaded and verified file: ../data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ../data/VOCtrainval_11-May-2012.tar to ../data\n"
          ]
        }
      ],
      "source": [
        "dataset1 = datasets.VOCSegmentation(\n",
        "    \"../data\",\n",
        "    year=\"2012\",\n",
        "    image_set=\"train\",\n",
        "    download=True,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "dataset2 = datasets.VOCSegmentation(\n",
        "    \"../data\",\n",
        "    year=\"2012\",\n",
        "    image_set=\"val\",\n",
        "    download=True,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **train_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1KtqXIPRXfJ",
        "outputId": "a4c76c6c-3237-4720-e210-3559a1250a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1464 (0%)]\tLoss: -0.073585\n",
            "Train Epoch: 1 [1280/1464 (83%)]\tLoss: -4.263477\n",
            "\n",
            "Test set: Average loss: -6.8388, Accuracy: 10243034/72705024 (14%)\n",
            "\n",
            "Train Epoch: 2 [0/1464 (0%)]\tLoss: -5.277812\n",
            "Train Epoch: 2 [1280/1464 (83%)]\tLoss: -11.558469\n",
            "\n",
            "Test set: Average loss: -9.6302, Accuracy: 26423992/72705024 (36%)\n",
            "\n",
            "Train Epoch: 3 [0/1464 (0%)]\tLoss: -13.128027\n",
            "Train Epoch: 3 [1280/1464 (83%)]\tLoss: -22.129061\n",
            "\n",
            "Test set: Average loss: -27.7508, Accuracy: 24170424/72705024 (33%)\n",
            "\n",
            "Train Epoch: 4 [0/1464 (0%)]\tLoss: -23.972858\n",
            "Train Epoch: 4 [1280/1464 (83%)]\tLoss: -37.629009\n",
            "\n",
            "Test set: Average loss: -40.2392, Accuracy: 29938542/72705024 (41%)\n",
            "\n",
            "Train Epoch: 5 [0/1464 (0%)]\tLoss: -40.107445\n",
            "Train Epoch: 5 [1280/1464 (83%)]\tLoss: -58.106251\n",
            "\n",
            "Test set: Average loss: -96.7993, Accuracy: 32579009/72705024 (45%)\n",
            "\n",
            "Train Epoch: 6 [0/1464 (0%)]\tLoss: -61.710720\n",
            "Train Epoch: 6 [1280/1464 (83%)]\tLoss: -84.702011\n",
            "\n",
            "Test set: Average loss: -105.8778, Accuracy: 38209030/72705024 (53%)\n",
            "\n",
            "Train Epoch: 7 [0/1464 (0%)]\tLoss: -89.665726\n",
            "Train Epoch: 7 [1280/1464 (83%)]\tLoss: -117.086975\n",
            "\n",
            "Test set: Average loss: -117.9616, Accuracy: 37627747/72705024 (52%)\n",
            "\n",
            "Train Epoch: 8 [0/1464 (0%)]\tLoss: -122.725426\n",
            "Train Epoch: 8 [1280/1464 (83%)]\tLoss: -156.284943\n",
            "\n",
            "Test set: Average loss: -165.3110, Accuracy: 43180070/72705024 (59%)\n",
            "\n",
            "Train Epoch: 9 [0/1464 (0%)]\tLoss: -162.601807\n",
            "Train Epoch: 9 [1280/1464 (83%)]\tLoss: -201.057983\n",
            "\n",
            "Test set: Average loss: -167.0719, Accuracy: 43890259/72705024 (60%)\n",
            "\n",
            "Train Epoch: 10 [0/1464 (0%)]\tLoss: -209.274628\n",
            "Train Epoch: 10 [1280/1464 (83%)]\tLoss: -253.784454\n",
            "\n",
            "Test set: Average loss: -248.5108, Accuracy: 46040088/72705024 (63%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = UNet(\n",
        "    encoder_channels=[3, 8, 16, 32],\n",
        "    decoder_channels=[32, 16, 8],\n",
        "    num_classes=num_classes,\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "GifpCp-rRXfJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "Y2Aa03GTRXfJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 ('gsn-JLMYFmkM')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b24dd471e30d41a13314bcb0a6607aa895817efc34b906f957cb683f2502811d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
