{"cells":[{"cell_type":"markdown","metadata":{"id":"84VetyCaGLyR"},"source":["<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n","\n","AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n","<hr>\n","\n","<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n","\n","<center>\n","Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n","Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n","Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n","Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n","    </center>"]},{"cell_type":"markdown","metadata":{"id":"ziZ9i7tXbO1T"},"source":["In this lab, you will implement some of the techniques discussed in the lecture.\n","\n","Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n"," * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n"," * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence \n","\n","**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two function as a single block and not even compute the gradient over the softmax values. \n","\n","**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n","\n","**Task 3 (optional).** Implement Adagrad, dropout and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.\n","\n","**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1668258012105,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"P22HqX9AbO1a"},"outputs":[],"source":["import random\n","import numpy as np\n","from torchvision import datasets, transforms"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1668258012632,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"N9jGPaZhbO2B"},"outputs":[],"source":["# Let's read the mnist dataset\n","\n","def load_mnist(path='.'):\n","    train_set = datasets.MNIST(path, train=True, download=True)\n","    x_train = train_set.data.numpy()\n","    _y_train = train_set.targets.numpy()\n","    \n","    test_set = datasets.MNIST(path, train=False, download=True)\n","    x_test = test_set.data.numpy()\n","    _y_test = test_set.targets.numpy()\n","    \n","    x_train = x_train.reshape((x_train.shape[0],28*28)) / 255.\n","    x_test = x_test.reshape((x_test.shape[0],28*28)) / 255.\n","\n","    y_train = np.zeros((_y_train.shape[0], 10))\n","    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n","    \n","    y_test = np.zeros((_y_test.shape[0], 10))\n","    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1668258012634,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"w3gAyqw4bO1p"},"outputs":[],"source":["def sigmoid(z):\n","    return 1.0/(1.0+np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    # Derivative of the sigmoid\n","    return sigmoid(z)*(1-sigmoid(z))\n","\n","def softmax(z):\n","    # Numericaly stable\n","    exps = np.exp(z - np.max(z))\n","    sum = np.sum(exps, axis=0)\n","    res = exps / sum\n","    return res"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.matmul(w, a)+b)\n","        return a\n","    \n","    def update_mini_batch(self, mini_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch which is as in tensorflow API.\n","        # eta is the learning rate      \n","        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n","            \n","        self.weights = [w-(eta/len(mini_batch[0]))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(mini_batch[0]))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","        \n","    def backprop(self, x, y):\n","        # For a single input (x,y) return a pair of lists.\n","        # First contains gradients over biases, second over weights.\n","        g = x\n","        gs = [g] # list to store all the gs, layer by layer\n","        fs = [] # list to store all the fs, layer by layer\n","        for b, w in zip(self.biases, self.weights):\n","            f = np.dot(w, g)+b\n","            fs.append(f)\n","            g = sigmoid(f)\n","            gs.append(g)\n","        # backward pass <- both steps at once\n","        dLdg = self.cost_derivative(gs[-1], y)\n","        dLdfs = []\n","        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n","            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n","            dLdfs.append(dLdf)\n","            dLdg = np.matmul(w.T, dLdf)\n","        \n","        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] \n","        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] \n","        return (dLdBs,dLdWs)\n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1668258012635,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"FgEA2XRRbO2X"},"outputs":[],"source":["class MyNetwork(object):\n","    def __init__(self, sizes, lambda_l2=0.0, momentum=0.0, dropout_p=0.0):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.lambda_l2 = lambda_l2\n","        self.momentum = momentum\n","        self.dropout_p = dropout_p\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.weights_momentum = [np.zeros_like(x) for x in self.weights]\n","        self.biases_momentum = [np.zeros_like(x) for x in self.biases]\n","    \n","    def activation(self, z, l):\n","        if l == (self.num_layers - 1):\n","            return softmax(z)\n","        else:\n","            return sigmoid(z)\n","    \n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        for l, b, w in zip(range(1, self.num_layers), self.biases, self.weights):\n","            if l < self.num_layers - 1:\n","                a = self.activation((1.0-self.dropout_p)*(np.matmul(w, a)+b), l)\n","            else:\n","                a = self.activation(np.matmul(w, a)+b, l)\n","        return a\n","    \n","    def update_mini_batch(self, mini_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch which is as in tensorflow API.\n","        # eta is the learning rate      \n","        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n","        \n","        ### Momentum equation for parameter p\n","        ### p_(t+1) = p_t + m_(t+1)\n","        ### m_(t+1) = lambda_momentum * m_t - eta * gradient(p_t)\n","        self.weights_momentum = [(self.momentum*wm)-(eta/len(mini_batch[0]))*nw \n","                                 for wm, w, nw in zip(self.weights_momentum, self.weights, nabla_w)]\n","        self.biases_momentum = [(self.momentum*bm)-(eta/len(mini_batch[0]))*nb \n","                                for bm, b, nb in zip(self.biases_momentum, self.biases, nabla_b)]\n","                            \n","        self.weights = [w+wm for w, wm in zip(self.weights, self.weights_momentum)]\n","        self.biases = [b+bm for b, bm in zip(self.biases, self.biases_momentum)]\n","        \n","    def backprop(self, x, y):\n","        # For a single input (x,y) return a pair of lists.\n","        # First contains gradients over biases, second over weights.\n","        ### Dropout\n","        dropout = [np.random.choice([0,1], size=(k,x.shape[1]), p=[self.dropout_p, 1-self.dropout_p]) for k in self.sizes[:-1]]\n","        dropout.append(np.full((self.sizes[-1], x.shape[1]), 1))\n","\n","        g = np.multiply(dropout[0], x)\n","        gs = [g] # list to store all the gs, layer by layer\n","        fs = [] # list to store all the fs, layer by layer\n","        for l, b, w in zip(range(1, self.num_layers), self.biases, self.weights):\n","            f = np.multiply(dropout[l], (np.dot(w, g)+b))\n","            fs.append(f)\n","            g = self.activation(f, l)\n","            gs.append(g)\n","        # backward pass <- both steps at once\n","        ### we know that [logloss(softmax)]' ~= [mse]'\n","        ### change dLdg to dLdf -> considering logloss and softmax as a single block\n","        ### the rest of dLdf calculations can be left as they were\n","        dLdf = self.cost_derivative(gs[-1], y)\n","        dLdfs = []\n","        for l,w,g in reversed(list(zip(range(1, self.num_layers), self.weights, gs[1:]))):\n","            if l < (self.num_layers - 1):\n","                dLdf = np.multiply(np.multiply(dLdg, np.multiply(g,1-g)), dropout[l])\n","            dLdfs.append(dLdf)\n","            dLdg = np.matmul(w.T, dLdf)\n","        \n","        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] \n","        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] \n","\n","        ### L2 regularization\n","        ### loss = old_loss + lambda_l2 * sum(weights^2)\n","        dLdWs = [dLdW + (self.lambda_l2 * w) for dLdW, w in zip(dLdWs, self.weights)]\n","\n","        return (dLdBs,dLdWs)\n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Accuracy: 0.6442\n","Epoch: 1, Accuracy: 0.7708\n","Epoch: 2, Accuracy: 0.7934\n","Epoch: 3, Accuracy: 0.8056\n","Epoch: 4, Accuracy: 0.814\n","Epoch: 5, Accuracy: 0.8202\n","Epoch: 6, Accuracy: 0.8252\n","Epoch: 7, Accuracy: 0.8278\n","Epoch: 8, Accuracy: 0.8309\n","Epoch: 9, Accuracy: 0.8337\n","CPU times: user 2min 58s, sys: 2min 26s, total: 5min 25s\n","Wall time: 31.5 s\n"]}],"source":["%%time\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=10, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15606,"status":"ok","timestamp":1668258028228,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"BKBj9u_cX6-O","outputId":"8914e09e-a8dd-4454-804d-7da955660fd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Accuracy: 0.8877\n","Epoch: 1, Accuracy: 0.9119\n","Epoch: 2, Accuracy: 0.9248\n","Epoch: 3, Accuracy: 0.9336\n","Epoch: 4, Accuracy: 0.9391\n","Epoch: 5, Accuracy: 0.9424\n","Epoch: 6, Accuracy: 0.9445\n","Epoch: 7, Accuracy: 0.9471\n","Epoch: 8, Accuracy: 0.9495\n","Epoch: 9, Accuracy: 0.9504\n","CPU times: user 9min 47s, sys: 9min 34s, total: 19min 22s\n","Wall time: 2min\n"]}],"source":["%%time\n","\n","network = MyNetwork([784,30,10], lambda_l2=0.01, momentum=0.5)\n","network.SGD((x_train, y_train), epochs=10, mini_batch_size=100, eta=1.0, test_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1668258028230,"user":{"displayName":"Marcin Mike","userId":"13029352554716653846"},"user_tz":-60},"id":"zuQPEfssqAir"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Accuracy: 0.8962\n","Epoch: 1, Accuracy: 0.9198\n","Epoch: 2, Accuracy: 0.9349\n","Epoch: 3, Accuracy: 0.9403\n","Epoch: 4, Accuracy: 0.9465\n","Epoch: 5, Accuracy: 0.9097\n","Epoch: 6, Accuracy: 0.9221\n","Epoch: 7, Accuracy: 0.9616\n","Epoch: 8, Accuracy: 0.9588\n","Epoch: 9, Accuracy: 0.9591\n","CPU times: user 10min 46s, sys: 10min 12s, total: 20min 59s\n","Wall time: 1min 57s\n"]}],"source":["%%time\n","\n","network = MyNetwork([784,100,30,10], lambda_l2=0.01, momentum=0.1, dropout_p=0.1)\n","network.SGD((x_train, y_train), epochs=10, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1_Jq1pNGB5t8q-uE6pKsGkAwN1GvsXSZ7","timestamp":1668007676457},{"file_id":"1mbRybTuEd5hfMgPq47jAy40aizNX03x8","timestamp":1665425459792},{"file_id":"1hs2ViNkY7vFE7l_PL7b-2XIN17cxbsyL","timestamp":1635823858600},{"file_id":"1t76la2tUWVLnEK7IKxdFZn_Y0be3xZud","timestamp":1635823610862}]},"kernelspec":{"display_name":"Python 3.10.8 ('gsn-JLMYFmkM')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b24dd471e30d41a13314bcb0a6607aa895817efc34b906f957cb683f2502811d"}}},"nbformat":4,"nbformat_minor":0}
